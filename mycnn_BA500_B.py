import torch
import numpy as np
from torch.utils.data import TensorDataset, DataLoader
import os
import random

import torch.nn as nn
import torchvision
import matplotlib.pyplot as plt
import cv2
import torch.optim as optim
import torch.nn.functional as F
from PIL import Image
from torchvision import datasets, transforms
from torchvision.utils import save_image

from ExampleNet import ExampleNet
from ExampleNet import CNN
from ExampleNet import CNN_500
from ExampleNet import CNN_500_part
torch.set_printoptions(profile='full')
# my_x = [np.array([[1.0,2],[3,4]]),np.array([[5.,6],[7,8]])] # a list of numpy arrays
# my_y = [np.array([4.]), np.array([2.])] # another list of numpy arrays (targets)
# tensor_x = torch.Tensor(my_x) # transform to torch tensor
# tensor_y = torch.Tensor(my_y)
bmname = 'BA500_p0.5_5k4_m50'
#_c0 and _c0test;;;_c0train and _c0ttest
def read_directory(directory_name):
    array_of_img = []  # this if for store all of the image data
    files = os.listdir(r"./"+directory_name)
    files.sort(key=lambda x:int(x[:-4]))                 ###########一切都是有原因的！！！顺序错了导致对应labels错了导致结果不好
    #files.sort()                                         # 单个100-199是对的，但和0-99组合在一起就不对了,此处改正后重新训练0-99
    for filename in files:
        img = cv2.imread(directory_name + "/" + filename,-1)
        array_of_img.append(img)
        #print(img)
    array_of_img=np.array(array_of_img)
    return array_of_img


def pre_data(bmname):
    prefix = os.path.join('cnn_data_dw_3', bmname,bmname)
    filename_classlabel = prefix + '_graph_labels.txt'
    graph_label = []
    label_vals = []
    with open(filename_classlabel, 'r') as f1:
        for line in f1:
            line = line.strip("\n")
            val = int(line)
            if val not in label_vals:
                label_vals.append(val)
            graph_label.append(val)
    label_map_to_int = {val: i for i, val in enumerate(label_vals)}  #将标签变为按顺序的从0-n
    graph_label = np.array([label_map_to_int[l] for l in graph_label])
    tensor_label = torch.Tensor(graph_label).long()

    #adj_data = np.load(prefix+'_adj.npy')
    org_img_folder = prefix + '_adjdata'
    adj_data = read_directory(org_img_folder)
    tensor_data = torch.Tensor(adj_data).short()
    return tensor_data, tensor_label

#my_dataset = TensorDataset(tensor_data,tensor_label) # create your datset
#my_dataloader = DataLoader(my_dataset) # create your dataloader
#dataloader = torch.utils.data.DataLoader(my_dataset, batch_size=4, shuffle=True)

# class0= {0: 0, 1: 1, 3: 2, 5: 3, 6: 4, 202: 5, 203: 6, 229: 7, 230: 8, 231: 9, 326: 10, 327: 11, 328: 12, 329: 13, 330: 14, 345: 15, 346: 16, 347: 17, 348: 18, 368: 19, 399: 20, 400: 21, 402: 22, 403: 23, 404: 24, 415: 25, 450: 26, 451: 27, 471: 28}
# class1= {2: 0, 4: 1, 10: 2, 11: 3, 12: 4, 24: 5, 25: 6, 26: 7, 27: 8, 28: 9, 43: 10, 54: 11, 65: 12, 66: 13, 92: 14, 93: 15, 94: 16, 95: 17, 96: 18, 98: 19, 105: 20, 172: 21, 174: 22, 179: 23, 180: 24, 181: 25, 182: 26, 183: 27, 184: 28, 186: 29, 188: 30, 205: 31, 207: 32, 208: 33, 239: 34, 240: 35, 243: 36, 244: 37, 245: 38, 246: 39, 249: 40, 257: 41, 260: 42, 261: 43, 262: 44, 274: 45, 284: 46, 286: 47, 287: 48, 289: 49, 290: 50, 291: 51, 295: 52, 298: 53, 324: 54, 361: 55, 363: 56, 382: 57, 390: 58, 393: 59, 398: 60, 409: 61, 410: 62, 411: 63, 413: 64, 423: 65, 424: 66, 425: 67, 426: 68, 428: 69, 429: 70, 430: 71, 435: 72, 455: 73, 472: 74, 474: 75, 478: 76, 479: 77}
# class2= {226: 0, 372: 1, 487: 2}
# class3= {13: 0, 14: 1, 15: 2, 16: 3, 18: 4, 19: 5, 20: 6, 21: 7, 22: 8, 23: 9, 85: 10, 191: 11, 192: 12, 193: 13, 194: 14, 383: 15, 394: 16, 449: 17}
# class4= {17: 0, 77: 1, 79: 2, 110: 3, 111: 4, 161: 5, 162: 6, 163: 7, 164: 8, 166: 9, 167: 10, 168: 11, 169: 12, 171: 13, 173: 14, 247: 15, 252: 16, 297: 17, 369: 18, 370: 19, 391: 20, 416: 21, 417: 22, 418: 23, 419: 24, 458: 25, 459: 26, 473: 27, 486: 28, 490: 29}
# class5= {187: 0, 320: 1, 321: 2, 414: 3, 491: 4}
# class6= {31: 0, 47: 1, 51: 2, 52: 3, 53: 4, 55: 5, 56: 6, 57: 7, 58: 8, 59: 9, 60: 10, 61: 11, 62: 12, 63: 13, 64: 14, 67: 15, 68: 16, 69: 17, 70: 18, 71: 19, 72: 20, 73: 21, 80: 22, 82: 23, 83: 24, 97: 25, 104: 26, 107: 27, 108: 28, 109: 29, 134: 30, 141: 31, 165: 32, 170: 33, 175: 34, 176: 35, 177: 36, 178: 37, 209: 38, 223: 39, 225: 40, 227: 41, 232: 42, 233: 43, 234: 44, 235: 45, 236: 46, 241: 47, 258: 48, 259: 49, 263: 50, 265: 51, 268: 52, 271: 53, 272: 54, 273: 55, 275: 56, 276: 57, 300: 58, 301: 59, 335: 60, 353: 61, 364: 62, 374: 63, 384: 64, 385: 65, 386: 66, 461: 67, 470: 68, 475: 69, 485: 70, 495: 71}
# class7= {211: 0, 212: 1, 213: 2, 214: 3, 215: 4, 444: 5}
# class8= {38: 0, 112: 1, 113: 2, 116: 3, 118: 4, 119: 5, 120: 6, 121: 7, 122: 8, 124: 9, 126: 10, 127: 11, 128: 12, 129: 13, 130: 14, 131: 15, 132: 16, 135: 17, 136: 18, 137: 19, 139: 20, 145: 21, 146: 22, 147: 23, 148: 24, 149: 25, 150: 26, 151: 27, 152: 28, 153: 29, 154: 30, 155: 31, 156: 32, 157: 33, 158: 34, 159: 35, 160: 36, 189: 37, 216: 38, 217: 39, 218: 40, 254: 41, 255: 42, 277: 43, 278: 44, 288: 45, 314: 46, 317: 47, 318: 48, 319: 49, 343: 50, 344: 51, 373: 52, 397: 53, 405: 54, 406: 55, 407: 56, 437: 57, 462: 58, 463: 59, 464: 60, 465: 61, 480: 62, 483: 63, 488: 64, 489: 65, 492: 66, 494: 67, 496: 68, 497: 69, 498: 70, 499: 71}
# class9= {39: 0, 40: 1, 41: 2, 42: 3, 44: 4, 45: 5, 114: 6, 115: 7, 219: 8, 220: 9, 221: 10, 222: 11, 322: 12}
# class10= {33: 0, 34: 1, 35: 2, 36: 3, 37: 4, 46: 5, 48: 6, 49: 7, 50: 8, 75: 9, 76: 10, 78: 11, 308: 12, 309: 13, 310: 14, 311: 15, 312: 16, 313: 17, 336: 18, 337: 19, 338: 20, 339: 21, 378: 22, 379: 23, 387: 24, 388: 25, 389: 26, 396: 27, 412: 28, 432: 29, 469: 30, 481: 31}
# class11= {7: 0, 8: 1, 9: 2, 29: 3, 30: 4, 32: 5, 81: 6, 84: 7, 86: 8, 87: 9, 88: 10, 89: 11, 90: 12, 91: 13, 106: 14, 117: 15, 123: 16, 125: 17, 133: 18, 138: 19, 140: 20, 142: 21, 143: 22, 144: 23, 185: 24, 190: 25, 195: 26, 196: 27, 197: 28, 198: 29, 199: 30, 200: 31, 201: 32, 206: 33, 210: 34, 224: 35, 228: 36, 237: 37, 238: 38, 242: 39, 250: 40, 251: 41, 253: 42, 266: 43, 279: 44, 280: 45, 285: 46, 296: 47, 307: 48, 316: 49, 323: 50, 331: 51, 332: 52, 333: 53, 334: 54, 340: 55, 341: 56, 342: 57, 351: 58, 352: 59, 354: 60, 355: 61, 356: 62, 357: 63, 358: 64, 359: 65, 360: 66, 362: 67, 365: 68, 366: 69, 375: 70, 381: 71, 392: 72, 401: 73, 408: 74, 422: 75, 427: 76, 431: 77, 433: 78, 434: 79, 436: 80, 438: 81, 439: 82, 440: 83, 441: 84, 442: 85, 443: 86, 445: 87, 446: 88, 447: 89, 448: 90, 456: 91, 457: 92, 460: 93, 466: 94, 467: 95, 468: 96, 477: 97, 482: 98, 484: 99}
# class12= {74: 0, 99: 1, 100: 2, 101: 3, 102: 4, 103: 5, 204: 6, 248: 7, 256: 8, 264: 9, 267: 10, 269: 11, 270: 12, 281: 13, 282: 14, 283: 15, 292: 16, 293: 17, 294: 18, 299: 19, 302: 20, 303: 21, 304: 22, 305: 23, 306: 24, 315: 25, 325: 26, 349: 27, 350: 28, 367: 29, 371: 30, 376: 31, 377: 32, 380: 33, 395: 34, 420: 35, 421: 36, 452: 37, 453: 38, 454: 39, 476: 40, 493: 41}
# class0 = {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38, 39: 39, 40: 40, 41: 41, 42: 42, 43: 43, 44: 44, 45: 45, 46: 46, 47: 47, 48: 48, 49: 49, 50: 50, 51: 51, 52: 52, 53: 53, 54: 54, 55: 55, 56: 56, 57: 57, 58: 58, 59: 59, 60: 60, 61: 61, 62: 62, 63: 63, 64: 64, 65: 65, 66: 66, 67: 67, 68: 68, 69: 69, 70: 70, 71: 71, 72: 72, 73: 73, 74: 74, 75: 75, 76: 76, 77: 77, 78: 78, 79: 79, 80: 80, 81: 81, 82: 82, 83: 83, 84: 84, 85: 85, 86: 86, 87: 87, 88: 88, 89: 89, 90: 90, 91: 91, 92: 92, 93: 93, 94: 94, 95: 95, 96: 96, 97: 97, 98: 98, 99: 99}
# class1 = {100: 0, 101: 1, 102: 2, 103: 3, 104: 4, 105: 5, 106: 6, 107: 7, 108: 8, 109: 9, 110: 10, 111: 11, 112: 12, 113: 13, 114: 14, 115: 15, 116: 16, 117: 17, 118: 18, 119: 19, 120: 20, 121: 21, 122: 22, 123: 23, 124: 24, 125: 25, 126: 26, 127: 27, 128: 28, 129: 29, 130: 30, 131: 31, 132: 32, 133: 33, 134: 34, 135: 35, 136: 36, 137: 37, 138: 38, 139: 39, 140: 40, 141: 41, 142: 42, 143: 43, 144: 44, 145: 45, 146: 46, 147: 47, 148: 48, 149: 49, 150: 50, 151: 51, 152: 52, 153: 53, 154: 54, 155: 55, 156: 56, 157: 57, 158: 58, 159: 59, 160: 60, 161: 61, 162: 62, 163: 63, 164: 64, 165: 65, 166: 66, 167: 67, 168: 68, 169: 69, 170: 70, 171: 71, 172: 72, 173: 73, 174: 74, 175: 75, 176: 76, 177: 77, 178: 78, 179: 79, 180: 80, 181: 81, 182: 82, 183: 83, 184: 84, 185: 85, 186: 86, 187: 87, 188: 88, 189: 89, 190: 90, 191: 91, 192: 92, 193: 93, 194: 94, 195: 95, 196: 96, 197: 97, 198: 98, 199: 99}
# class2 = {200: 0, 201: 1, 202: 2, 203: 3, 204: 4, 205: 5, 206: 6, 207: 7, 208: 8, 209: 9, 210: 10, 211: 11, 212: 12, 213: 13, 214: 14, 215: 15, 216: 16, 217: 17, 218: 18, 219: 19, 220: 20, 221: 21, 222: 22, 223: 23, 224: 24, 225: 25, 226: 26, 227: 27, 228: 28, 229: 29, 230: 30, 231: 31, 232: 32, 233: 33, 234: 34, 235: 35, 236: 36, 237: 37, 238: 38, 239: 39, 240: 40, 241: 41, 242: 42, 243: 43, 244: 44, 245: 45, 246: 46, 247: 47, 248: 48, 249: 49, 250: 50, 251: 51, 252: 52, 253: 53, 254: 54, 255: 55, 256: 56, 257: 57, 258: 58, 259: 59, 260: 60, 261: 61, 262: 62, 263: 63, 264: 64, 265: 65, 266: 66, 267: 67, 268: 68, 269: 69, 270: 70, 271: 71, 272: 72, 273: 73, 274: 74, 275: 75, 276: 76, 277: 77, 278: 78, 279: 79, 280: 80, 281: 81, 282: 82, 283: 83, 284: 84, 285: 85, 286: 86, 287: 87, 288: 88, 289: 89, 290: 90, 291: 91, 292: 92, 293: 93, 294: 94, 295: 95, 296: 96, 297: 97, 298: 98, 299: 99}
# class3 = {300: 0, 301: 1, 302: 2, 303: 3, 304: 4, 305: 5, 306: 6, 307: 7, 308: 8, 309: 9, 310: 10, 311: 11, 312: 12, 313: 13, 314: 14, 315: 15, 316: 16, 317: 17, 318: 18, 319: 19, 320: 20, 321: 21, 322: 22, 323: 23, 324: 24, 325: 25, 326: 26, 327: 27, 328: 28, 329: 29, 330: 30, 331: 31, 332: 32, 333: 33, 334: 34, 335: 35, 336: 36, 337: 37, 338: 38, 339: 39, 340: 40, 341: 41, 342: 42, 343: 43, 344: 44, 345: 45, 346: 46, 347: 47, 348: 48, 349: 49, 350: 50, 351: 51, 352: 52, 353: 53, 354: 54, 355: 55, 356: 56, 357: 57, 358: 58, 359: 59, 360: 60, 361: 61, 362: 62, 363: 63, 364: 64, 365: 65, 366: 66, 367: 67, 368: 68, 369: 69, 370: 70, 371: 71, 372: 72, 373: 73, 374: 74, 375: 75, 376: 76, 377: 77, 378: 78, 379: 79, 380: 80, 381: 81, 382: 82, 383: 83, 384: 84, 385: 85, 386: 86, 387: 87, 388: 88, 389: 89, 390: 90, 391: 91, 392: 92, 393: 93, 394: 94, 395: 95, 396: 96, 397: 97, 398: 98, 399: 99}
# class4 = {400: 0, 401: 1, 402: 2, 403: 3, 404: 4, 405: 5, 406: 6, 407: 7, 408: 8, 409: 9, 410: 10, 411: 11, 412: 12, 413: 13, 414: 14, 415: 15, 416: 16, 417: 17, 418: 18, 419: 19, 420: 20, 421: 21, 422: 22, 423: 23, 424: 24, 425: 25, 426: 26, 427: 27, 428: 28, 429: 29, 430: 30, 431: 31, 432: 32, 433: 33, 434: 34, 435: 35, 436: 36, 437: 37, 438: 38, 439: 39, 440: 40, 441: 41, 442: 42, 443: 43, 444: 44, 445: 45, 446: 46, 447: 47, 448: 48, 449: 49, 450: 50, 451: 51, 452: 52, 453: 53, 454: 54, 455: 55, 456: 56, 457: 57, 458: 58, 459: 59, 460: 60, 461: 61, 462: 62, 463: 63, 464: 64, 465: 65, 466: 66, 467: 67, 468: 68, 469: 69, 470: 70, 471: 71, 472: 72, 473: 73, 474: 74, 475: 75, 476: 76, 477: 77, 478: 78, 479: 79, 480: 80, 481: 81, 482: 82, 483: 83, 484: 84, 485: 85, 486: 86, 487: 87, 488: 88, 489: 89, 490: 90, 491: 91, 492: 92, 493: 93, 494: 94, 495: 95, 496: 96, 497: 97, 498: 98, 499: 99}
#
class0= {0: 0, 2: 1, 4: 2, 5: 3, 6: 4, 8: 5, 18: 6, 24: 7, 25: 8, 31: 9, 33: 10, 46: 11, 60: 12, 63: 13, 67: 14, 69: 15, 71: 16, 78: 17, 87: 18, 88: 19, 90: 20, 91: 21, 100: 22, 113: 23, 114: 24, 120: 25, 127: 26, 133: 27, 135: 28, 140: 29, 142: 30, 151: 31, 153: 32, 154: 33, 156: 34, 161: 35, 165: 36, 167: 37, 179: 38, 183: 39, 186: 40, 201: 41, 203: 42, 204: 43, 205: 44, 213: 45, 214: 46, 222: 47, 223: 48, 225: 49, 230: 50, 231: 51, 235: 52, 237: 53, 240: 54, 249: 55, 250: 56, 259: 57, 261: 58, 273: 59, 280: 60, 282: 61, 288: 62, 289: 63, 290: 64, 295: 65, 297: 66, 300: 67, 301: 68, 303: 69, 304: 70, 309: 71, 311: 72, 312: 73, 314: 74, 319: 75, 321: 76, 330: 77, 333: 78, 345: 79, 346: 80, 348: 81, 354: 82, 355: 83, 356: 84, 358: 85, 367: 86, 369: 87, 373: 88, 376: 89, 377: 90, 378: 91, 379: 92, 383: 93, 386: 94, 387: 95, 393: 96, 396: 97, 398: 98, 402: 99, 413: 100, 414: 101, 419: 102, 421: 103, 422: 104, 424: 105, 427: 106, 429: 107, 431: 108, 438: 109, 439: 110, 445: 111, 448: 112, 449: 113, 455: 114, 458: 115, 459: 116, 464: 117, 469: 118, 482: 119, 486: 120, 490: 121, 495: 122, 496: 123, 499: 124}
class1= {10: 0, 28: 1, 30: 2, 44: 3, 50: 4, 52: 5, 55: 6, 56: 7, 59: 8, 61: 9, 66: 10, 77: 11, 81: 12, 95: 13, 97: 14, 110: 15, 111: 16, 116: 17, 117: 18, 122: 19, 123: 20, 152: 21, 160: 22, 170: 23, 171: 24, 177: 25, 178: 26, 182: 27, 185: 28, 189: 29, 199: 30, 217: 31, 218: 32, 224: 33, 226: 34, 229: 35, 238: 36, 242: 37, 243: 38, 247: 39, 252: 40, 256: 41, 264: 42, 265: 43, 271: 44, 272: 45, 276: 46, 277: 47, 279: 48, 287: 49, 298: 50, 299: 51, 308: 52, 315: 53, 324: 54, 325: 55, 343: 56, 344: 57, 352: 58, 357: 59, 361: 60, 362: 61, 381: 62, 382: 63, 385: 64, 389: 65, 395: 66, 401: 67, 412: 68, 462: 69, 465: 70, 483: 71}
class2= {7: 0, 9: 1, 19: 2, 20: 3, 22: 4, 27: 5, 34: 6, 38: 7, 39: 8, 41: 9, 42: 10, 62: 11, 68: 12, 73: 13, 75: 14, 83: 15, 93: 16, 98: 17, 99: 18, 101: 19, 102: 20, 105: 21, 106: 22, 107: 23, 125: 24, 126: 25, 141: 26, 143: 27, 144: 28, 146: 29, 147: 30, 162: 31, 164: 32, 168: 33, 187: 34, 188: 35, 191: 36, 194: 37, 196: 38, 198: 39, 210: 40, 228: 41, 234: 42, 236: 43, 241: 44, 258: 45, 267: 46, 269: 47, 281: 48, 291: 49, 310: 50, 317: 51, 318: 52, 323: 53, 328: 54, 329: 55, 332: 56, 339: 57, 347: 58, 349: 59, 360: 60, 363: 61, 370: 62, 372: 63, 384: 64, 391: 65, 407: 66, 416: 67, 417: 68, 420: 69, 434: 70, 443: 71, 446: 72, 451: 73, 460: 74, 470: 75, 474: 76, 475: 77, 484: 78, 494: 79, 498: 80}
class3= {1: 0, 3: 1, 11: 2, 12: 3, 14: 4, 15: 5, 17: 6, 23: 7, 32: 8, 37: 9, 40: 10, 45: 11, 47: 12, 48: 13, 53: 14, 58: 15, 70: 16, 72: 17, 80: 18, 82: 19, 85: 20, 89: 21, 92: 22, 103: 23, 108: 24, 112: 25, 115: 26, 118: 27, 119: 28, 124: 29, 134: 30, 138: 31, 139: 32, 145: 33, 148: 34, 155: 35, 159: 36, 163: 37, 169: 38, 172: 39, 174: 40, 175: 41, 176: 42, 181: 43, 184: 44, 190: 45, 193: 46, 195: 47, 197: 48, 202: 49, 206: 50, 208: 51, 211: 52, 212: 53, 215: 54, 219: 55, 221: 56, 233: 57, 239: 58, 244: 59, 246: 60, 248: 61, 253: 62, 257: 63, 260: 64, 266: 65, 270: 66, 275: 67, 285: 68, 286: 69, 292: 70, 302: 71, 305: 72, 306: 73, 307: 74, 313: 75, 320: 76, 322: 77, 331: 78, 336: 79, 337: 80, 338: 81, 342: 82, 351: 83, 353: 84, 359: 85, 364: 86, 365: 87, 371: 88, 374: 89, 375: 90, 390: 91, 392: 92, 399: 93, 405: 94, 406: 95, 410: 96, 418: 97, 425: 98, 428: 99, 432: 100, 433: 101, 436: 102, 437: 103, 440: 104, 442: 105, 447: 106, 450: 107, 456: 108, 463: 109, 467: 110, 468: 111, 471: 112, 472: 113, 473: 114, 479: 115, 481: 116, 485: 117, 487: 118, 488: 119, 489: 120, 491: 121, 492: 122, 493: 123}
class4= {13: 0, 16: 1, 21: 2, 26: 3, 29: 4, 35: 5, 36: 6, 43: 7, 49: 8, 51: 9, 54: 10, 57: 11, 64: 12, 65: 13, 74: 14, 76: 15, 79: 16, 84: 17, 86: 18, 94: 19, 96: 20, 104: 21, 109: 22, 121: 23, 128: 24, 129: 25, 130: 26, 131: 27, 132: 28, 136: 29, 137: 30, 149: 31, 150: 32, 157: 33, 158: 34, 166: 35, 173: 36, 180: 37, 192: 38, 200: 39, 207: 40, 209: 41, 216: 42, 220: 43, 227: 44, 232: 45, 245: 46, 251: 47, 254: 48, 255: 49, 262: 50, 263: 51, 268: 52, 274: 53, 278: 54, 283: 55, 284: 56, 293: 57, 294: 58, 296: 59, 316: 60, 326: 61, 327: 62, 334: 63, 335: 64, 340: 65, 341: 66, 350: 67, 366: 68, 368: 69, 380: 70, 388: 71, 394: 72, 397: 73, 400: 74, 403: 75, 404: 76, 408: 77, 409: 78, 411: 79, 415: 80, 423: 81, 426: 82, 430: 83, 435: 84, 441: 85, 444: 86, 452: 87, 453: 88, 454: 89, 457: 90, 461: 91, 466: 92, 476: 93, 477: 94, 478: 95, 480: 96, 497: 97}
#125 , 72 , 81 , 124 , 98 #BA500-5kp1
# class0= {48: 0, 51: 1, 53: 2, 54: 3, 55: 4, 56: 5, 57: 6, 58: 7, 59: 8, 60: 9, 61: 10, 62: 11, 63: 12, 64: 13, 65: 14, 66: 15, 67: 16, 68: 17, 69: 18, 70: 19, 71: 20, 72: 21, 102: 22, 103: 23, 104: 24, 105: 25, 106: 26, 108: 27, 109: 28, 139: 29, 141: 30, 142: 31, 143: 32, 144: 33, 158: 34, 159: 35, 160: 36, 161: 37, 162: 38, 163: 39, 164: 40, 165: 41, 166: 42, 167: 43, 168: 44, 169: 45, 170: 46, 171: 47, 172: 48, 173: 49, 174: 50, 175: 51, 176: 52, 177: 53, 178: 54, 179: 55, 180: 56, 181: 57, 182: 58, 183: 59, 184: 60, 185: 61, 186: 62, 187: 63, 188: 64, 189: 65, 190: 66, 192: 67, 194: 68, 195: 69, 196: 70, 197: 71, 198: 72, 199: 73, 276: 74, 277: 75, 279: 76, 280: 77, 282: 78, 283: 79, 284: 80, 285: 81, 286: 82, 287: 83, 290: 84, 291: 85, 292: 86, 293: 87, 316: 88, 327: 89, 328: 90, 329: 91, 330: 92, 331: 93, 332: 94, 333: 95, 334: 96, 335: 97, 336: 98, 337: 99, 338: 100, 339: 101, 340: 102, 352: 103, 404: 104, 405: 105, 406: 106, 407: 107, 408: 108, 442: 109, 464: 110, 465: 111, 466: 112, 467: 113, 468: 114, 469: 115, 470: 116, 471: 117, 472: 118, 473: 119, 474: 120, 475: 121}
# class1= {14: 0, 16: 1, 32: 2, 33: 3, 34: 4, 35: 5, 36: 6, 37: 7, 38: 8, 39: 9, 40: 10, 41: 11, 42: 12, 43: 13, 44: 14, 45: 15, 46: 16, 47: 17, 49: 18, 50: 19, 52: 20, 145: 21, 146: 22, 147: 23, 148: 24, 149: 25, 150: 26, 151: 27, 152: 28, 153: 29, 154: 30, 155: 31, 156: 32, 157: 33, 200: 34, 201: 35, 202: 36, 203: 37, 204: 38, 206: 39, 214: 40, 216: 41, 217: 42, 218: 43, 219: 44, 220: 45, 221: 46, 222: 47, 223: 48, 224: 49, 225: 50, 226: 51, 227: 52, 228: 53, 229: 54, 230: 55, 231: 56, 232: 57, 233: 58, 234: 59, 235: 60, 236: 61, 237: 62, 238: 63, 239: 64, 240: 65, 241: 66, 242: 67, 243: 68, 244: 69, 245: 70, 246: 71, 247: 72, 248: 73, 249: 74, 250: 75, 251: 76, 252: 77, 257: 78, 258: 79, 288: 80, 313: 81, 318: 82, 319: 83, 320: 84, 321: 85, 322: 86, 349: 87, 350: 88, 351: 89, 353: 90, 354: 91, 355: 92, 381: 93, 382: 94, 383: 95, 384: 96, 385: 97, 386: 98, 387: 99, 388: 100, 389: 101, 390: 102, 391: 103, 392: 104, 393: 105, 395: 106, 437: 107, 438: 108, 439: 109, 440: 110, 441: 111, 447: 112, 478: 113}
# class2= {107: 0, 110: 1, 111: 2, 112: 3, 113: 4, 114: 5, 115: 6, 116: 7, 117: 8, 118: 9, 119: 10, 120: 11, 121: 12, 122: 13, 123: 14, 124: 15, 125: 16, 126: 17, 127: 18, 128: 19, 129: 20, 130: 21, 131: 22, 132: 23, 133: 24, 134: 25, 135: 26, 136: 27, 137: 28, 138: 29, 140: 30, 208: 31, 209: 32, 210: 33, 211: 34, 212: 35, 213: 36, 215: 37, 259: 38, 260: 39, 261: 40, 262: 41, 263: 42, 264: 43, 265: 44, 266: 45, 267: 46, 268: 47, 269: 48, 270: 49, 271: 50, 272: 51, 273: 52, 274: 53, 370: 54, 476: 55, 477: 56, 479: 57, 480: 58, 481: 59, 482: 60, 483: 61, 484: 62, 485: 63, 486: 64, 487: 65, 488: 66, 490: 67, 492: 68}
# class3= {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 15: 14, 18: 15, 19: 16, 275: 17, 278: 18, 289: 19, 307: 20, 314: 21, 315: 22, 317: 23, 324: 24, 358: 25, 359: 26, 360: 27, 361: 28, 362: 29, 363: 30, 364: 31, 365: 32, 371: 33, 372: 34, 373: 35, 374: 36, 375: 37, 376: 38, 416: 39, 418: 40, 419: 41, 420: 42, 421: 43, 422: 44, 423: 45, 424: 46, 425: 47, 426: 48, 427: 49, 428: 50, 429: 51, 430: 52, 431: 53, 432: 54, 433: 55, 434: 56, 435: 57, 436: 58, 463: 59, 491: 60, 493: 61, 494: 62, 495: 63, 496: 64, 497: 65, 498: 66, 499: 67}
# class4= {17: 0, 20: 1, 21: 2, 22: 3, 23: 4, 24: 5, 25: 6, 26: 7, 27: 8, 28: 9, 29: 10, 30: 11, 31: 12, 73: 13, 74: 14, 75: 15, 76: 16, 77: 17, 78: 18, 79: 19, 80: 20, 81: 21, 82: 22, 83: 23, 84: 24, 85: 25, 86: 26, 87: 27, 88: 28, 89: 29, 90: 30, 91: 31, 92: 32, 93: 33, 94: 34, 95: 35, 96: 36, 97: 37, 98: 38, 99: 39, 100: 40, 101: 41, 191: 42, 193: 43, 205: 44, 207: 45, 253: 46, 254: 47, 255: 48, 256: 49, 281: 50, 294: 51, 295: 52, 296: 53, 297: 54, 298: 55, 299: 56, 300: 57, 301: 58, 302: 59, 303: 60, 304: 61, 305: 62, 306: 63, 308: 64, 309: 65, 310: 66, 311: 67, 312: 68, 323: 69, 325: 70, 326: 71, 341: 72, 342: 73, 343: 74, 344: 75, 345: 76, 346: 77, 347: 78, 348: 79, 356: 80, 357: 81, 366: 82, 367: 83, 368: 84, 369: 85, 377: 86, 378: 87, 379: 88, 380: 89, 394: 90, 396: 91, 397: 92, 398: 93, 399: 94, 400: 95, 401: 96, 402: 97, 403: 98, 409: 99, 410: 100, 411: 101, 412: 102, 413: 103, 414: 104, 415: 105, 417: 106, 443: 107, 444: 108, 445: 109, 446: 110, 448: 111, 449: 112, 450: 113, 451: 114, 452: 115, 453: 116, 454: 117, 455: 118, 456: 119, 457: 120, 458: 121, 459: 122, 460: 123, 461: 124, 462: 125, 489: 126}
# #122 , 114 , 69 , 68 , 127  #WS500_5kp1
class0_reverse = {value: i for i, value in class0.items()}
class1_reverse = {value: i for i, value in class1.items()}
class2_reverse = {value: i for i, value in class2.items()}
class3_reverse = {value: i for i, value in class3.items()}
class4_reverse = {value: i for i, value in class4.items()}
# class5_reverse = {value: i for i, value in class5.items()}
# class6_reverse = {value: i for i, value in class6.items()}
# class7_reverse = {value: i for i, value in class7.items()}
# class8_reverse = {value: i for i, value in class8.items()}
# class9_reverse = {value: i for i, value in class9.items()}
# class10_reverse = {value: i for i, value in class10.items()}
# class11_reverse = {value: i for i, value in class11.items()}
# class12_reverse = {value: i for i, value in class12.items()}
def main():

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    epochs = 5

    batch_size = 20
    num_class= 98
    #net = ExampleNet().to(device)
    net = CNN_500_part(num_class).to(device)
    print(net)
    criterion = nn.MSELoss(reduce = None, size_average = None)
    cross = nn.CrossEntropyLoss()
    optimizer = optim.Adam(net.parameters())

    train_data, train_label = pre_data(bmname+'_train')
    train_data = torch.unsqueeze(train_data, dim=1).type(torch.FloatTensor)
    train_dataset = TensorDataset(train_data, train_label)
    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    test_data, test_label = pre_data(bmname+'_test')
    #test_data, test_label = pre_data('food500_SI_A3_m6_test')
    test_data = torch.unsqueeze(test_data, dim=1).type(torch.FloatTensor)
    test_dataset = TensorDataset(test_data, test_label)
    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)

    losses = []
    for i in range(epochs):

        for j, (input, target) in enumerate(train_dataloader):
            input = input.to(device)
            output = net(input)
            target = torch.zeros(target.size(0), num_class).scatter_(1, target.view(-1, 1), 1).to(device)
            _, indices = torch.max(target, 1)
            #loss = criterion(output, target)
            loss = cross(output,indices)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            if j % 10 == 0:
                losses.append(loss.float().detach().numpy())
                print("[epochs - {0} - {1}/{2}]loss: {3}".format(i, j, len(train_dataloader), loss.float()))
                #plt.clf()
                #plt.plot(losses)
                #plt.savefig(bmname+'0_loss.jpg')
                #plt.pause(0.01)
        with torch.no_grad():
            net.eval()
            correct = 0.
            total = 0.
            labels= []
            preds=[]
            for input, target in test_dataloader:
                input, target = input.to(device), target.to(device)
                output = net(input)
                _, predicted = torch.max(output.data, 1)
                total += target.size(0)
                correct += (predicted == target).sum()
                accuracy = correct.float() / total
                labels.append(target.long().numpy())
                preds.append(predicted.long().numpy())
            net.train()
            print("[epochs - {0}]Accuracy:{1}%".format(i + 1, (100 * accuracy)))

            labels = np.hstack(labels)
            preds = np.hstack(preds)
            print('labels:',labels)
            print("preds:",preds)
            real_labels = []
            for n in labels:
                real_labels.append(class4_reverse[n])
            real_preds = []
            for m in preds:
                real_preds.append(class4_reverse[m])
            read_dic = np.load("BA500_short_path.npy", allow_pickle=True).item()
            distance_pred = []
            count = 0
            for w in range(len(labels)):
                a = read_dic[real_labels[w]][real_preds[w]]
                distance_pred.append(a)
                if real_labels[w] == real_preds[w]:
                    count = count + 1
            print('perdict accuracy:', count / len(labels))
            result_dis = {}
            ave_class = 0
            for q in set(distance_pred):
                result_dis[q] = distance_pred.count(q)
                ave_class = q * result_dis[q] + ave_class
            print('GNN distance:', ave_class / len(labels))
        torch.save(net.state_dict(), "models/"+bmname+"_n2.pth")
    para = sum([np.prod(list(p.size())) for p in net.parameters()])
    print(para)
    print('Model {} : params: {:4f}M'.format(net._get_name(), para * 4 / 1000 / 1000))
if __name__ == "__main__":
    main()


